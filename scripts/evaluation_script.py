#!/usr/bin/env python3
# scripts/evaluation_script.py
import sys
import os
import torch
import mlflow
import pandas as pd
import numpy as np
from datetime import datetime
from datasets import load_dataset
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu
from rouge_score import rouge_scorer
import nltk
import json

# Add the project root to the path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import modules
from src.fine_tuning import (
    load_model_and_tokenizer,
    generate_response
)
# Import utility modules
from utils.mlflow_utils import (
    mlflow_log_model_info,
    mlflow_start_run,
    mlflow_setup_tracking
)
from utils.yaml_utils import (
    load_config
)
from utils.constants import RESULTS_DIR
from utils.system_utils import configure_device_settings

from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Download NLTK resources if needed
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("Downloading NLTK punkt tokenizer...")
    nltk.download('punkt', quiet=False)

# Define a simple tokenizer that doesn't rely on punkt_tab
def simple_tokenize(text):
    """
    Simple tokenization function that works without punkt_tab.
    Split by whitespace and punctuation.
    """
    if not isinstance(text, str):
        return []
    # Basic whitespace tokenization
    return text.lower().split()

def calculate_bleu(references, hypotheses):
    """
    Calculate BLEU score for the given references and hypotheses.
    
    Args:
        references: List of reference sentences (tokenized)
        hypotheses: List of model generated sentences (tokenized)
        
    Returns:
        Dict containing BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores
    """
    # For corpus BLEU, we need list of lists (each reference is a list of tokens)
    references_for_corpus = [[ref] for ref in references]
    
    # Calculate BLEU-1 to BLEU-4
    bleu1 = corpus_bleu(
        references_for_corpus, 
        hypotheses, 
        weights=(1, 0, 0, 0)
    )
    bleu2 = corpus_bleu(
        references_for_corpus, 
        hypotheses, 
        weights=(0.5, 0.5, 0, 0)
    )
    bleu3 = corpus_bleu(
        references_for_corpus, 
        hypotheses, 
        weights=(0.33, 0.33, 0.33, 0)
    )
    bleu4 = corpus_bleu(
        references_for_corpus, 
        hypotheses, 
        weights=(0.25, 0.25, 0.25, 0.25)
    )
    
    return {
        "bleu1": bleu1,
        "bleu2": bleu2,
        "bleu3": bleu3,
        "bleu4": bleu4
    }

def calculate_rouge(references, hypotheses):
    """
    Calculate ROUGE scores for the given references and hypotheses.
    
    Args:
        references: List of reference sentences (not tokenized)
        hypotheses: List of model generated sentences (not tokenized)
        
    Returns:
        Dict containing ROUGE-1, ROUGE-2, and ROUGE-L scores
    """
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    
    scores = {
        'rouge1_precision': 0.0,
        'rouge1_recall': 0.0,
        'rouge1_fmeasure': 0.0,
        'rouge2_precision': 0.0,
        'rouge2_recall': 0.0,
        'rouge2_fmeasure': 0.0,
        'rougeL_precision': 0.0,
        'rougeL_recall': 0.0,
        'rougeL_fmeasure': 0.0
    }
    
    for ref, hyp in zip(references, hypotheses):
        rouge_scores = scorer.score(ref, hyp)
        
        # Accumulate scores
        scores['rouge1_precision'] += rouge_scores['rouge1'].precision
        scores['rouge1_recall'] += rouge_scores['rouge1'].recall
        scores['rouge1_fmeasure'] += rouge_scores['rouge1'].fmeasure
        
        scores['rouge2_precision'] += rouge_scores['rouge2'].precision
        scores['rouge2_recall'] += rouge_scores['rouge2'].recall
        scores['rouge2_fmeasure'] += rouge_scores['rouge2'].fmeasure
        
        scores['rougeL_precision'] += rouge_scores['rougeL'].precision
        scores['rougeL_recall'] += rouge_scores['rougeL'].recall
        scores['rougeL_fmeasure'] += rouge_scores['rougeL'].fmeasure
    
    # Calculate average scores
    n = len(references)
    for key in scores:
        scores[key] /= n
    
    return scores

def evaluate_model(model, tokenizer, test_dataset, config):
    """
    Evaluate the model using BLEU and ROUGE metrics.
    
    Args:
        model: The fine-tuned model
        tokenizer: Tokenizer for the model
        test_dataset: Dataset to evaluate on
        config: Configuration dictionary
        
    Returns:
        Dict containing evaluation metrics
    """
    # Get test data
    instructions = test_dataset[config.get('data', {}).get('instruction_column', 'instruction')]
    references = test_dataset[config.get('data', {}).get('response_column', 'response')]
    
    # Generate responses
    hypotheses = []
    print(f"Generating responses for {len(instructions)} test examples...")
    for i, instruction in enumerate(instructions):
        if i % 10 == 0:
            print(f"  Progress: {i}/{len(instructions)}")
        response = generate_response(instruction, model, tokenizer)
        hypotheses.append(response)
    
    # Save generations for inspection
    results_df = pd.DataFrame({
        'instruction': instructions,
        'reference': references,
        'generated': hypotheses
    })
    
    # Tokenize for BLEU
    tokenized_references = []
    tokenized_hypotheses = []
    
    print("Tokenizing text for evaluation...")
    for ref, hyp in zip(references, hypotheses):
        try:
            # Use simple_tokenize instead of nltk.word_tokenize
            tokenized_ref = simple_tokenize(ref)
        except Exception as e:
            print(f"Error tokenizing reference: {e}")
            tokenized_ref = []
            
        try:
            # Use simple_tokenize instead of nltk.word_tokenize
            tokenized_hyp = simple_tokenize(hyp)
        except Exception as e:
            print(f"Error tokenizing hypothesis: {e}")
            tokenized_hyp = []
            
        tokenized_references.append(tokenized_ref)
        tokenized_hypotheses.append(tokenized_hyp)
    
    # Calculate metrics
    print("Calculating BLEU scores...")
    bleu_scores = calculate_bleu(tokenized_references, tokenized_hypotheses)
    
    print("Calculating ROUGE scores...")
    rouge_scores = calculate_rouge(references, hypotheses)
    
    # Combine metrics
    metrics = {**bleu_scores, **rouge_scores}
    
    return metrics, results_df

def main():
    # Load configuration
    config = load_config()
    
    # Set up MLflow tracking
    tracking_uri = mlflow_setup_tracking(config)
    
    # Generate a run ID and set up directories
    run_timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    run_name = f"chatbot_evaluation_{run_timestamp}"
    output_dir = os.path.join(RESULTS_DIR, "evaluations", run_timestamp)
    
    # Create necessary directories
    os.makedirs(output_dir, exist_ok=True)
    
    # Start MLflow run
    with mlflow_start_run(run_name) as run:
        run_id = run.info.run_id
        print(f"MLflow run ID: {run_id}")
        
        # Configure device settings
        device_config = configure_device_settings(config)
        
        # Get model path or name
        model_name = config.get('evaluation', {}).get('model_path', None)
        if not model_name:
            print("Error: Model path not specified in config. Add 'model_path' under 'evaluation' section.")
            return
        
        # Load model and tokenizer
        print(f"Loading model: {model_name}")
        model, tokenizer = load_model_and_tokenizer(
            model_name,
            load_in_8bit=device_config["use_8bit"],
            torch_dtype=device_config["torch_dtype"],
            device_map=device_config["device_map"]
        )
        
        # Log model info
        mlflow_log_model_info(model)
        
        # Load test dataset
        eval_dataset_path = config.get('evaluation', {}).get('eval_dataset_path')
        if not eval_dataset_path:
            # If not specified, use the same dataset as training but only test split
            eval_dataset_path = config.get('data', {}).get('dataset_path')
            if not eval_dataset_path:
                print("Error: No dataset path specified in config.")
                return
            
            # Load and split dataset
            print(f"Loading dataset: {eval_dataset_path}")
            dataset = load_dataset('csv', data_files=eval_dataset_path)['train']
            test_dataset = dataset.train_test_split(
                test_size=config.get('data', {}).get('test_size', 0.1), 
                seed=config.get('data', {}).get('seed', 42)
            )["test"]
        else:
            # Load the specified test dataset
            print(f"Loading test dataset: {eval_dataset_path}")
            test_dataset = load_dataset('csv', data_files=eval_dataset_path)['train']
        
        print(f"Test dataset size: {len(test_dataset)}")
        
        # Evaluate model
        print("Evaluating model...")
        metrics, results_df = evaluate_model(model, tokenizer, test_dataset, config)
        
        # Log metrics to MLflow
        print("Logging metrics to MLflow...")
        mlflow.log_metrics(metrics)
        
        # Save metrics to JSON for DVC
        metrics_dir = os.path.join(RESULTS_DIR)
        os.makedirs(metrics_dir, exist_ok=True)
        metrics_path = os.path.join(metrics_dir, "metrics.json")
        with open(metrics_path, 'w') as f:
            json.dump(metrics, f, indent=2)
        print(f"Metrics saved to {metrics_path} for DVC tracking")
        
        # Save results dataframe
        results_path = os.path.join(output_dir, "generation_results.csv")
        results_df.to_csv(results_path, index=False)
        mlflow.log_artifact(results_path)
        
        # Print metrics
        print("\nEvaluation Metrics:")
        for metric_name, metric_value in metrics.items():
            print(f"  {metric_name}: {metric_value:.4f}")
        
        print(f"\nEvaluation completed successfully.")
        print(f"Results saved to: {results_path}")
        print(f"MLflow run ID: {run_id}")
        print(f"MLflow tracking URI: {tracking_uri}")

if __name__ == "__main__":
    main()